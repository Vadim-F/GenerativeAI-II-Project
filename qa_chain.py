# qa_chain.py ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å –±–∞–∑–æ–π —á–µ—Ä–µ–∑ ChromaDB + Gemini (RAG —Å –ø–∞–º—è—Ç—å—é —á–µ—Ä–µ–∑ ConversationBufferMemory –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º –¥–∏–∞–ª–æ–≥–æ–º)

import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langsmith import traceable
from langchain_core.tracers import LangChainTracer

# üîê –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env
load_dotenv()

# üß† –ó–∞–≥—Ä—É–∂–∞–µ–º ChromaDB —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(
    persist_directory="chroma_store",
    embedding_function=embedding_model
)
retriever = vectorstore.as_retriever()

# üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="chat_history"
)

# üìú –®–∞–±–ª–æ–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∏—Å—Ç–æ—Ä–∏–µ–π
prompt = PromptTemplate.from_template("""
–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å.

–ò—Å—Ç–æ—Ä–∏—è:
{chat_history}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:
""")

# ü§ñ LLM Gemini Flash
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.3
)

# üîó –¶–µ–ø–æ—á–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
chain = (
    RunnableLambda(lambda x: {
        "context": retriever.get_relevant_documents(x["question"]),
        "question": x["question"],
        "chat_history": memory.load_memory_variables({})["chat_history"]
    })
    | (lambda x: prompt.format(**x))
    | llm
)

# üí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
print("üîé –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):")
while True:
    question = input("\nüß† –í–∞—à –≤–æ–ø—Ä–æ—Å: ")
    if question.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
        print("üëã –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
        break
    
    # –°–æ–∑–¥–∞—ë–º —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤—â–∏–∫
    tracer = LangChainTracer()
    
    # –ó–∞–ø—É—Å–∫ —Å —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–æ–π
    result = chain.invoke(
    {"question": question},
    config={"callbacks": [tracer]})
    print("üìÑ –û—Ç–≤–µ—Ç:", result.content)

    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø–∞–º—è—Ç—å –≤—Ä—É—á–Ω—É—é (—Å–∏–º—É–ª–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏)
    memory.chat_memory.add_user_message(question)
    memory.chat_memory.add_ai_message(result.content)