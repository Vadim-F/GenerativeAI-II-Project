# qa_chain.py ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å –±–∞–∑–æ–π —á–µ—Ä–µ–∑ ChromaDB + Gemini

import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

# üîê –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env
load_dotenv()

# üß† –ó–∞–≥—Ä—É–∂–∞–µ–º ChromaDB —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(
    persist_directory="chroma_store",
    embedding_function=embedding_model
)
retriever = vectorstore.as_retriever()

# üìú –®–∞–±–ª–æ–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
prompt = PromptTemplate.from_template("""
–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî —Å–∫–∞–∂–∏ —á–µ—Å—Ç–Ω–æ, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
---------
{context}
---------

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:
""")

# ü§ñ LLM Gemini Flash
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    temperature=0.3
)

# üîó –¶–µ–ø–æ—á–∫–∞: –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Üí —à–∞–±–ª–æ–Ω ‚Üí –º–æ–¥–µ–ª—å
chain = (
    RunnableLambda(lambda x: {
        "context": retriever.get_relevant_documents(x["question"]),
        "question": x["question"]
    })
    | (lambda x: prompt.format(**x))
    | llm
)

# üí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
print("üîé –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è):")
while True:
    question = input("\nüß† –í–∞—à –≤–æ–ø—Ä–æ—Å: ")
    if question.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
        print("üëã –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
        break
    result = chain.invoke({"question": question})
    print("üìÑ –û—Ç–≤–µ—Ç:", result.content)
